{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import socket\n",
    "import ssl\n",
    "from urllib.request import Request, urlopen, URLError, HTTPError, ProxyHandler, build_opener, install_opener\n",
    "from urllib.parse import quote, urlsplit, urlunsplit, urlencode\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 YaBrowser/19.6.1.153 Yowser/2.5 Safari/537.36'\n",
    "PROXIES = {'https': 'https://37.203.246.48:42911'}\n",
    "MIN_TIME_SLEEP = 1\n",
    "MAX_TIME_SLEEP = 3\n",
    "MAX_COUNTS = 5\n",
    "TIMEOUT = 10\n",
    "URL_BASE = 'https://www.pfl-russia.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_PATH = './_data/{}/'.format('pfl')\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.makedirs(CACHE_PATH)\n",
    "CACHE_PATH_CLDS = '{}clds/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_CLDS):\n",
    "    os.makedirs(CACHE_PATH_CLDS)\n",
    "CACHE_PATH_CLDS_RAW = '{}clds_raw/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_CLDS_RAW):\n",
    "    os.makedirs(CACHE_PATH_CLDS_RAW)\n",
    "CACHE_PATH_CLDS_RFRS = '{}clds_rfrs/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_CLDS_RFRS):\n",
    "    os.makedirs(CACHE_PATH_CLDS_RFRS)\n",
    "CACHE_PATH_CLDS_RFRS_RAW = '{}clds_rfrs_raw/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_CLDS_RFRS_RAW):\n",
    "    os.makedirs(CACHE_PATH_CLDS_RFRS_RAW)\n",
    "CACHE_PATH_GMS = '{}gms/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_GMS):\n",
    "    os.makedirs(CACHE_PATH_GMS)\n",
    "CACHE_PATH_GMS_RAW = '{}gms_raw/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_GMS_RAW):\n",
    "    os.makedirs(CACHE_PATH_GMS_RAW)\n",
    "CACHE_PATH_TMS = '{}tms/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_TMS):\n",
    "    os.makedirs(CACHE_PATH_TMS)\n",
    "CACHE_PATH_TMS_RAW = '{}tms_raw/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_TMS_RAW):\n",
    "    os.makedirs(CACHE_PATH_TMS_RAW)\n",
    "CACHE_PATH_PLRS = '{}plrs/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_PLRS):\n",
    "    os.makedirs(CACHE_PATH_PLRS)\n",
    "CACHE_PATH_PLRS_RAW = '{}plrs_raw/'.format(CACHE_PATH)\n",
    "if not os.path.exists(CACHE_PATH_PLRS_RAW):\n",
    "    os.makedirs(CACHE_PATH_PLRS_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url_page, timeout, proxies, file=False):\n",
    "    counts = 0\n",
    "    content = None\n",
    "    while counts < MAX_COUNTS:\n",
    "        try:\n",
    "            request = Request(url_page)\n",
    "            request.add_header('User-Agent', USER_AGENT)\n",
    "            proxy_support = ProxyHandler(proxies)\n",
    "            opener = build_opener(proxy_support)\n",
    "            install_opener(opener)\n",
    "            context = ssl._create_unverified_context()\n",
    "            response = urlopen(request, context=context, timeout=timeout)\n",
    "            if file:\n",
    "                content = response.read()\n",
    "            else:\n",
    "                content =  response.read().decode(response.headers.get_content_charset())\n",
    "            break\n",
    "        except URLError as e:\n",
    "            counts += 1\n",
    "            print('URLError | ', url_page, ' | ', e, ' | counts: ', counts)\n",
    "            sleep(randint(counts * MIN_TIME_SLEEP, counts * MAX_TIME_SLEEP))\n",
    "        except HTTPError as e:\n",
    "            counts += 1\n",
    "            print('HTTPError | ', url_page, ' | ', e, ' | counts: ', counts)\n",
    "            sleep(randint(counts * MIN_TIME_SLEEP, counts * MAX_TIME_SLEEP))\n",
    "        except socket.timeout as e:\n",
    "            counts += 1\n",
    "            print('socket timeout | ', url_page, ' | ', e, ' | counts: ', counts)\n",
    "            sleep(randint(counts * MIN_TIME_SLEEP, counts * MAX_TIME_SLEEP))\n",
    "    return content\n",
    "def translit(text):\n",
    "    symbols = ('абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ /-',\n",
    "               'abvgdeejzijklmnoprstufhccss_yieuaABVGDEEJZIJKLMNOPRSTUFHCCSS_YIEUA___')\n",
    "    tr = {ord(a):ord(b) for a, b in zip(*symbols)}\n",
    "    return text.translate(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = get_content(URL_BASE, TIMEOUT, PROXIES)\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "urls_clds = {}\n",
    "for itm in soup.find_all('a', {'class': False, 'aria-haspopup': False}):\n",
    "    if 'competitions/season' in itm['href']:\n",
    "        url_ssn = URL_BASE + itm['href']\n",
    "        html = get_content(url_ssn, TIMEOUT, PROXIES)\n",
    "        soup_i = BeautifulSoup(html, 'lxml')\n",
    "        for a in soup_i.find('div', {'class': \"seasons-select\"}).find_all('a'):\n",
    "            urls_clds.update({\n",
    "                itm.text.strip() + ' '+ a.text.strip(): URL_BASE + a['href']\n",
    "            })\n",
    "print(urls_clds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cld_name, url_cld in urls_clds.items():\n",
    "    print('processing: ', cld_name, ' | url: ', url_cld)\n",
    "    html = get_content(url_cld, TIMEOUT, PROXIES)\n",
    "    file_name = '{}calendar_{}.html'.format(CACHE_PATH_CLDS_RAW, translit(cld_name))\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(html))\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    date, tour, time = '', '', ''\n",
    "    cld_games = []\n",
    "    game = {}\n",
    "    for itm in soup.find('table', {'class': \"games-table\"}).find_all('tr'):\n",
    "        game = {}\n",
    "        if itm.find_all('td', {'class': False, 'colspan': True}):\n",
    "            date = itm.find('td', {'colspan': True}).text.strip()\n",
    "        if itm.find_all('td', {'class': \"games-date-tr\", 'colspan': True}):\n",
    "            tour = itm.text.strip()\n",
    "        if itm.find('span', {'class': \"match-date\"}):\n",
    "            time = itm.find('span', {'class': \"match-date\"}).text.strip()\n",
    "        if len(itm.find_all('td')) > 2:\n",
    "            game['date'] = date\n",
    "            game['tour'] = tour\n",
    "            game['time'] = time\n",
    "            game['team-home'] = itm.find_all('td')[1].text.strip()\n",
    "            if itm.find_all('td')[1].find('a'):\n",
    "                game['team-home_href'] = URL_BASE + itm.find_all('td')[1].find('a')['href']\n",
    "            game['game-score'] = ' '.join(itm.find_all('td')[2].text.split())\n",
    "            if itm.find_all('td')[2].find('a'):\n",
    "                game['game-score_href'] = URL_BASE + itm.find_all('td')[2].find('a')['href']\n",
    "            game['team-away'] = itm.find_all('td')[3].text.strip()\n",
    "            if itm.find_all('td')[3].find('a'):\n",
    "                game['team-away_href'] = URL_BASE + itm.find_all('td')[3].find('a')['href']\n",
    "            cld_games.append(game)\n",
    "    file_name = '{}calendar_{}.txt'.format(CACHE_PATH_CLDS, translit(cld_name))\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(cld_games, file)\n",
    "    print('done: ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cld_name, url_cld in urls_clds.items():\n",
    "    ref_name = cld_name + '_referees'\n",
    "    url_ref = url_cld.replace('calendar/', 'set-ref/')\n",
    "    print('processing: ', ref_name, ' | url: ', url_ref)\n",
    "    html = get_content(url_ref, TIMEOUT, PROXIES)\n",
    "    file_name = '{}calendar_{}.html'.format(CACHE_PATH_CLDS_RFRS_RAW, translit(ref_name))\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(html))\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    date, tour, time = '', '', ''\n",
    "    rfr_games = []\n",
    "    game = {}\n",
    "    for itm in soup.find('table', {'class': \"games-table for-judges\"}).find_all('tr'):\n",
    "        game = {}\n",
    "        if itm.find_all('td', {'class': False, 'colspan': \"8\"}):\n",
    "            date = itm.find('td', {'colspan': True}).text.strip()\n",
    "        if itm.find_all('td', {'class': \"games-date-tr\", 'colspan': True}):\n",
    "            tour = itm.text.strip()\n",
    "        if itm.find_all('td', {'class': \"header\"}):\n",
    "            header = []\n",
    "            for td in itm.find_all('td', {'class': \"header\"}):\n",
    "                header.append(td.text.strip())\n",
    "        if len(itm.find_all('td')) > 6:\n",
    "            game['date'] = date\n",
    "            game['tour'] = tour\n",
    "            game['header'] = header\n",
    "            game['game_id'] = itm.find('span', {'class': \"match-date\"}).find('a').text.strip()\n",
    "            game['game_id_href'] = itm.find('span', {'class': \"match-date\"}).find('a')['href']\n",
    "            for itd, td in enumerate(itm.find_all('td', {'class': \"jName\"})):\n",
    "                if td.find('span', {'class': \"jCity\"}):\n",
    "                    city = td.find('span', {'class': \"jCity\"}).text.strip()\n",
    "                else: \n",
    "                    city = ''\n",
    "                game['{}_{}'.format(itd, header[itd])] = {\n",
    "                    'name': td.text.replace(city, '').strip(),\n",
    "                    'city': city\n",
    "                }\n",
    "            rfr_games.append(game)\n",
    "    file_name = '{}calendar_{}.txt'.format(CACHE_PATH_CLDS_RFRS, translit(ref_name))\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(rfr_games, file)\n",
    "    print('done: ', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_urls = []\n",
    "teams_urls = []\n",
    "for file_name in os.listdir(CACHE_PATH_CLDS):\n",
    "    with open('{}{}'.format(CACHE_PATH_CLDS, file_name), 'r') as file:\n",
    "        data = json.load(file)\n",
    "    for game in data:\n",
    "        for k_game, v_game in game.items():\n",
    "            if 'score_href' in k_game:\n",
    "                games_urls.append(v_game)\n",
    "            elif '_href' in k_game:\n",
    "                teams_urls.append(v_game)\n",
    "            else:\n",
    "                pass\n",
    "print('games total: ', len(games_urls), ' | unique games: ', len(set(games_urls)))\n",
    "print('teams total: ', len(teams_urls), ' | unique teams: ', len(set(teams_urls)))\n",
    "games_urls = sorted(list(set(games_urls)))\n",
    "teams_urls = sorted(list(set(teams_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_index = len(os.listdir(CACHE_PATH_GMS))\n",
    "for game_url in games_urls[start_index:]:\n",
    "    print('processing: ', game_url)\n",
    "    html = get_content(game_url, TIMEOUT, PROXIES)\n",
    "    file_name = '{}{}.html'.format(CACHE_PATH_GMS_RAW, game_url[game_url.find('/game') + 1 : -1].replace('-', '_'))\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(html))\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    game = {}\n",
    "    #---overall info---\n",
    "    if soup.find('div', {'class': \"game-info\"}).find('h3'):\n",
    "        game['game_num'] = soup.find('div', {'class': \"game-info\"}).find('h3').text.strip()\n",
    "    game['description'] = ' '.join([\n",
    "        ' '.join(x.text.split()) for x in soup.find('div', {'class': \"game-info\"}).find_all('div', {'class': False})\n",
    "    ])\n",
    "    team_block = {}\n",
    "    for team_num, team in enumerate(soup.find_all('div', {'class': \"team-block\"})):\n",
    "        team_data = {}\n",
    "        if team.find('a', {'class': \"team-title\"}):\n",
    "            team_data['name'] = team.find('a', {'class': \"team-title\"}).text.strip()\n",
    "            team_data['city'] = team.find('a', {'class': \"team-city\"}).text.strip()\n",
    "        team_block.update({'team_' + str(team_num + 1): team_data})\n",
    "    game['team_block'] = team_block\n",
    "    if soup.find('div', {'class': \"game-score-label\"}):\n",
    "        game['game_score'] = soup.find('div', {'class': \"game-score-label\"}).text.strip()\n",
    "    #---text events---\n",
    "    text_events = []\n",
    "    header = []\n",
    "    if soup.find('table', {'class': \"text-events\"}):\n",
    "        for th in soup.find('table', {'class': \"text-events\"}).find_all('th'):\n",
    "            header.append(th.text.strip())\n",
    "        for tr in soup.find('table', {'class': \"text-events\"}).find('tbody').find_all('tr'):\n",
    "            event = {}\n",
    "            for itd, td in enumerate(tr.find_all('td')):\n",
    "                if td.find('div', {'class': True}):\n",
    "                    event_text = td.find('div')['class']\n",
    "                else:\n",
    "                    event_text = ' '.join(td.text.split())\n",
    "                event[header[itd]] = event_text\n",
    "                if td.find('a', {'href': True}):\n",
    "                    event[header[itd] + '_href'] = URL_BASE + td.find('a', {'href': True})['href']\n",
    "            text_events.append(event)\n",
    "    game['text_events'] = text_events\n",
    "    #---team players---\n",
    "    teams = {}\n",
    "    teams_lbl = ['teamA', 'teamB']\n",
    "    players_lbl = ['main', 'reserve']\n",
    "    if soup.find_all('div', {'class': \"teams-tables\"}):\n",
    "        for lbl in teams_lbl:\n",
    "            team = {}\n",
    "            for imr, mr in enumerate(players_lbl):\n",
    "                players = []\n",
    "                header = []\n",
    "                team_table = soup.find_all('div', {'class': \"teams-tables\"})[imr]\n",
    "                for th in team_table.find('div', {'class': lbl}).find('thead').find_all('th'):\n",
    "                    header.append(th.text.strip())\n",
    "                for tr in team_table.find('div', {'class': lbl}).find('tbody').find_all('tr'):\n",
    "                    player = {}\n",
    "                    for itd, td in enumerate(tr.find_all('td')):\n",
    "                        player_text = ' '.join(td.text.split())\n",
    "                        player[header[itd]] = player_text\n",
    "                        if td.find('div', {'class': True}):\n",
    "                            for iadd, add in enumerate(td.find_all('div', {'class': True})):\n",
    "                                player[header[itd] + '_' + str(iadd)] = add['class']\n",
    "                        if td.find('a', {'href': True}):\n",
    "                            player[header[itd] + '_href'] = URL_BASE + td.find('a', {'href': True})['href']\n",
    "                    players.append(player)\n",
    "                team[mr] = players\n",
    "            teams[lbl] = team\n",
    "    game['teams'] = teams\n",
    "    #---staff---\n",
    "    staff = {}\n",
    "    for lbl in teams_lbl:\n",
    "        coaches = {}\n",
    "        staff_team = []\n",
    "        header = ['title', 'name']\n",
    "        if soup.find('h3', {'class': \"h3 gameH3\"}):\n",
    "            if 'Тренер' in soup.find('h3', {'class': \"h3 gameH3\"}).text:\n",
    "                if len(soup.find_all('div', {'class': \"teams-tables\"})) >= 3:\n",
    "                    for tr in soup.find_all('div', {'class': \"teams-tables\"})[2].find('tbody').find_all('tr'):\n",
    "                        staffer = {}\n",
    "                        for itd, td in enumerate(tr.find_all('td')):\n",
    "                            staffer.update({header[itd]: td.text.strip()})\n",
    "                        staff_team.append(staffer)\n",
    "            staff[lbl] = staff_team\n",
    "    game['teams_staff'] = staff\n",
    "    #---officials---\n",
    "    officials = []\n",
    "    header = ['title', 'name', 'city']\n",
    "    if soup.find('table', {'class': \"game-judjes table-datails\"}):\n",
    "        for tr in soup.find('table', {'class': \"game-judjes table-datails\"}).find('tbody').find_all('tr'):\n",
    "            official = []\n",
    "            for itd, td in enumerate(tr.find_all('td')):\n",
    "                official.append({header[itd]: td.text.strip()})\n",
    "            officials.append(official)\n",
    "    game['officials'] = officials\n",
    "    #---save to file---\n",
    "    file_name = '{}{}.txt'.format(CACHE_PATH_GMS, game_url[game_url.find('/game') + 1 : -1].replace('-', '_'))\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(game, file)\n",
    "    print('done: ', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_index = len(os.listdir(CACHE_PATH_TMS))\n",
    "for team_url in teams_urls[start_index:]:\n",
    "    print('processing: ', team_url)\n",
    "    html = get_content(team_url, TIMEOUT, PROXIES)\n",
    "    file_name = '{}team_{}.html'.format(\n",
    "        CACHE_PATH_TMS_RAW, \n",
    "        team_url[team_url.find('/teams') + len('/teams') + 1 : -1].replace('-', '_').replace('/', '_')\n",
    "    )\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(html))\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    team = {}\n",
    "    #---overall info---\n",
    "    if soup.find('h1', {'class': \"club-header-name\"}):\n",
    "        team['club_name'] = soup.find('h1', {'class': \"club-header-name\"}).contents[0].strip()\n",
    "        team['seasons'] = soup.find('h1', {'class': \"club-header-name\"}).find('div', {'class': \"seasons-select\"}).contents[0].strip()\n",
    "        team['competitions'] = soup.find('h1', {'class': \"club-header-name\"}).find('a').text.strip()\n",
    "        team['competitions_href'] = URL_BASE + soup.find('h1', {'class': \"club-header-name\"}).find('a')['href']\n",
    "        for p in soup.find('div', {'class': \"club-header-info\"}).find_all('p'):\n",
    "            if p.find('a'):\n",
    "                team[p.find('span').text.strip()] = p.find('a').text.strip()\n",
    "            else:\n",
    "                team[p.find('span').text.strip()] = p.contents[1].strip()\n",
    "    #---players---\n",
    "    players = []\n",
    "    if soup.find('div', {'class': \"tab_team\"}):\n",
    "        for block in soup.find('div', {'class': \"tab_team\"}).find_all('div', {'class': \"row article-grid-container main-players team-players\"}):\n",
    "            block_players = []\n",
    "            for a in block.find_all('a', {'class': \"article\"}):\n",
    "                player = {}\n",
    "                block_players.append({'name': a.text.strip(), 'name_href': URL_BASE + a['href']})\n",
    "            players.append({block.find('h2').text.strip(): block_players})\n",
    "    team['players'] = players\n",
    "    #---officials---\n",
    "    if soup.find_all('table', {'class': \"leadership\"}):\n",
    "        if len(soup.find_all('table', {'class': \"leadership\"})) == 2:\n",
    "            officials = []\n",
    "            headers = []\n",
    "            for tr in soup.find_all('table', {'class': \"leadership\"})[0].find_all('tr'):\n",
    "                if tr.find_all('th'):\n",
    "                    for th in tr.find_all('th'):\n",
    "                        headers.append(th['class'][0] + '_' + th.text.strip() if th.text.strip() else th['class'][0])\n",
    "                if tr.find_all('td'):\n",
    "                    official = {}\n",
    "                    for itd, td in enumerate(tr.find_all('td')):\n",
    "                        official[headers[itd]] = td.text.strip()\n",
    "                    officials.append(official)\n",
    "            team['officials'] = officials\n",
    "            tbl_index = 1\n",
    "        else:\n",
    "            tbl_index = 0\n",
    "    #----coaches and staff---\n",
    "        coaches_and_staff = []\n",
    "        headers = []\n",
    "        for tr in soup.find_all('table', {'class': \"leadership\"})[tbl_index].find_all('tr'):\n",
    "            if tr.find_all('th'):\n",
    "                for th in tr.find_all('th'):\n",
    "                    headers.append(th['class'][0] + '_' + th.text.strip() if th.text.strip() else th['class'][0])\n",
    "            if tr.find_all('td'):\n",
    "                person = {}\n",
    "                for itd, td in enumerate(tr.find_all('td')):\n",
    "                    person[headers[itd]] = td.text.strip()\n",
    "                coaches_and_staff.append(person)\n",
    "    team['coaches_and_staff'] = coaches_and_staff\n",
    "    #---save to file---\n",
    "    file_name = '{}team_{}.txt'.format(\n",
    "        CACHE_PATH_TMS, \n",
    "        team_url[team_url.find('/teams') + len('/teams') + 1 : -1].replace('-', '_').replace('/', '_')\n",
    "    )\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(team, file)\n",
    "    print('done: ', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "players total:  47981  | unique players:  3648\n"
     ]
    }
   ],
   "source": [
    "plrs_urls = []\n",
    "for file_name in os.listdir(CACHE_PATH_GMS):\n",
    "    with open('{}{}'.format(CACHE_PATH_GMS, file_name), 'r') as file:\n",
    "        data = json.load(file)\n",
    "    for k1, v1 in data['teams'].items():\n",
    "        for k2, v2 in v1.items():\n",
    "            for item in v2:\n",
    "                for k3, v3 in item.items():\n",
    "                    if '_href' in k3:\n",
    "                        plrs_urls.append(v3)\n",
    "print('players total: ', len(plrs_urls), ' | unique players: ', len(set(plrs_urls)))\n",
    "plrs_urls = sorted(list(set(plrs_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.pfl-russia.com/players/10149/season-2018-2019/',\n",
       " 'https://www.pfl-russia.com/players/10149/season-2019-2020/',\n",
       " 'https://www.pfl-russia.com/players/10344/season-2018-2019/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plrs_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_index = len(os.listdir(CACHE_PATH_PLRS))\n",
    "for plr_url in plrs_urls[start_index:]:\n",
    "    print('processing: ', plr_url)\n",
    "    html = get_content(plr_url, TIMEOUT, PROXIES)\n",
    "    file_name = '{}player_{}.html'.format(\n",
    "        CACHE_PATH_PLRS_RAW, \n",
    "        plr_url[plr_url.find('/players') + len('/players') + 1 : -1].replace('-', '_').replace('/', '_')\n",
    "    )\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(html))\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    player = {}\n",
    "    #---overall info---\n",
    "    overall_data = soup.find('h1', {'class': \"club-header-name\"})\n",
    "    player['name'] = overall_data.contents[0].strip()\n",
    "    if overall_data.find('div', {'aria-expanded': \"false\"}):\n",
    "        player['season'] = overall_data.find('div', {'aria-expanded': \"false\"}).text.strip()\n",
    "    else:\n",
    "        player['season'] = overall_data.find('div', {'class': \"seasons-select\"}).contents[0].replace('Сезон ', '').strip()\n",
    "    for p in overall_data.find_all('p'):\n",
    "        if p.find('a'):\n",
    "            player[p.span.text.strip()] = p.find('a').text.strip()\n",
    "            player[p.span.text.strip() + '_href'] = URL_BASE + p.find('a')['href']\n",
    "        else:\n",
    "            player[p.span.text.strip()] = p.contents[1].strip()\n",
    "    #---player stats---\n",
    "    plr_stats = {}\n",
    "    for stat_tbl in soup.find_all('div', {'class': \"player-stat-by-team\"}):\n",
    "        plr_stats_team = []\n",
    "        headers = []\n",
    "        for th in stat_tbl.find('thead').find_all('th'):\n",
    "            headers.append(th.text.strip())\n",
    "        for tr in stat_tbl.find('tbody').find_all('tr'):\n",
    "            row = {}\n",
    "            for itd, td in enumerate(tr.find_all('td')):\n",
    "                row.update({headers[itd]: td.text.strip()})\n",
    "                if td.find('a'):\n",
    "                    row.update({headers[itd] + '_href': URL_BASE + td.find('a')['href']})\n",
    "            plr_stats_team.append(row)\n",
    "        if stat_tbl.find('div', {'class': \"h4\"}):\n",
    "            plr_stats[stat_tbl.find('div', {'class': \"h4\"}).text.strip()] = plr_stats_team\n",
    "        else:\n",
    "            plr_stats['default_team'] = plr_stats_team\n",
    "    player['statistics'] = plr_stats\n",
    "    #---save to file---\n",
    "    file_name = '{}player_{}.txt'.format(\n",
    "        CACHE_PATH_PLRS, \n",
    "        plr_url[plr_url.find('/players') + len('/players') + 1 : -1].replace('-', '_').replace('/', '_')\n",
    "    )\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(player, file)\n",
    "    print('done: ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
